{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation import Llama\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "os.environ['RANK'] = '0'  # Example rank, adjust accordingly\n",
    "os.environ['WORLD_SIZE'] = '1'  # Example world size, adjust accordingly\n",
    "os.environ['MASTER_ADDR'] = 'localhost'  # Example master address\n",
    "os.environ['MASTER_PORT'] = '12355'  # Example master port\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 13.32 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama = Llama.build(ckpt_dir= \"../llama3/Meta-Llama-3-8B-Instruct/\", tokenizer_path=\"../llama3/Meta-Llama-3-8B-Instruct/tokenizer.model\", max_seq_len= 512, max_batch_size = 4, model_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llama.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "prompt_template = '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(prompt: str):\n",
    "    return torch.tensor(tokenizer.encode(prompt_template.format(user_prompt = prompt), eos= False, bos = False), dtype= torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_tokens = tokenizer.encode_prompt(\"I came up with a new saying: 'stop and smell the roses' what you think of it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Please try to provide useful, helpful and actionable answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I came up with a new saying: 'stop and smell the roses' what you think of it<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 150/256 [00:05<00:03, 28.33it/s]\n"
     ]
    }
   ],
   "source": [
    "out_tokens = llama.generate([prompt_tokens], max_gen_len= 256, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I came up with a new saying: 'stop and smell the roses' what you think of it<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I think your new saying is... already a saying! \"Stop and smell the roses\" is a well-known English idiom that means to slow down and appreciate the small pleasures in life. It's often used to encourage people to take a break from their busy schedules and enjoy the beauty around them.\n",
      "\n",
      "That being said, it's still a lovely phrase, and I'm sure you came up with it independently! Idioms and phrases are often rephrased or reworded in different cultures and languages, so it's possible that your version is a unique twist on the original.\n",
      "\n",
      "If you'd like to create a new saying, I'd be happy to help you brainstorm some ideas. What theme or message would you like your saying to convey?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_tokens+ out_tokens[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CollectionHook\n",
    "\n",
    "hook = CollectionHook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "logits = model(prompt_tokens,0 , hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook.cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = {\"role\": \"user\", \"content\": \"Whats 1-1?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import ChatFormat\n",
    "\n",
    "formatter = ChatFormat(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = formatter.encode_dialog_prompt([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhats 1-1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27,\n",
       " 91,\n",
       " 7413,\n",
       " 3659,\n",
       " 4424,\n",
       " 91,\n",
       " 1822,\n",
       " 91,\n",
       " 2527,\n",
       " 8932,\n",
       " 851,\n",
       " 91,\n",
       " 29,\n",
       " 882,\n",
       " 27,\n",
       " 91,\n",
       " 408,\n",
       " 8932,\n",
       " 851,\n",
       " 91,\n",
       " 1363,\n",
       " 59175,\n",
       " 220,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 76514,\n",
       " 91,\n",
       " 68,\n",
       " 354,\n",
       " 851,\n",
       " 91,\n",
       " 1822,\n",
       " 91,\n",
       " 2527,\n",
       " 8932,\n",
       " 851,\n",
       " 91,\n",
       " 29,\n",
       " 78191,\n",
       " 27,\n",
       " 91,\n",
       " 408,\n",
       " 8932,\n",
       " 851,\n",
       " 91,\n",
       " 1363]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhats 1-1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prompt_tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhats 1-1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   27,    91,  7413,  3659,  4424,    91,  1822,    91,  2527,  8932,\n",
       "           851,    91,    29,   882,    27,    91,   408,  8932,   851,    91,\n",
       "          1363, 59175,   220,    16,    12,    16, 76514,    91,    68,   354,\n",
       "           851,    91,  1822,    91,  2527,  8932,   851,    91,    29, 78191,\n",
       "            27,    91,   408,  8932,   851,    91,  1363]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 <\n",
      "91 |\n",
      "7413 begin\n",
      "3659 _of\n",
      "4424 _text\n",
      "91 |\n",
      "1822 ><\n",
      "91 |\n",
      "2527 start\n",
      "8932 _header\n",
      "851 _id\n",
      "91 |\n",
      "29 >\n",
      "882 user\n",
      "27 <\n",
      "91 |\n",
      "408 end\n",
      "8932 _header\n",
      "851 _id\n",
      "91 |\n",
      "1363 >\n",
      "\n",
      "\n",
      "59175 Whats\n",
      "220  \n",
      "16 1\n",
      "12 -\n",
      "16 1\n",
      "76514 ?<\n",
      "91 |\n",
      "68 e\n",
      "354 ot\n",
      "851 _id\n",
      "91 |\n",
      "1822 ><\n",
      "91 |\n",
      "2527 start\n",
      "8932 _header\n",
      "851 _id\n",
      "91 |\n",
      "29 >\n",
      "78191 assistant\n",
      "27 <\n",
      "91 |\n",
      "408 end\n",
      "8932 _header\n",
      "851 _id\n",
      "91 |\n",
      "1363 >\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in prompt_tokens[0].tolist():\n",
    "    print(token, tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 <|begin_of_text|>\n",
      "128006 <|start_header_id|>\n",
      "882 user\n",
      "128007 <|end_header_id|>\n",
      "271 \n",
      "\n",
      "\n",
      "59175 Whats\n",
      "220  \n",
      "16 1\n",
      "12 -\n",
      "16 1\n",
      "30 ?\n",
      "128009 <|eot_id|>\n",
      "128006 <|start_header_id|>\n",
      "78191 assistant\n",
      "128007 <|end_header_id|>\n",
      "271 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token, tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Whats 1-1?\"\n",
    "tokens = []\n",
    "tokens.append(tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "tokens.extend(tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "tokens.append(tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "tokens.extend(tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "tokens.extend(\n",
    "    tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    ")\n",
    "tokens.append(tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
