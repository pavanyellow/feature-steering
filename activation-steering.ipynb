{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer_lens\n",
    "\n",
    "import transformer_lens\n",
    "from transformer_lens import utils\n",
    "import torch\n",
    "import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567c184a55f94e178ab43cdf9b913a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d5d2286fdb445eb38f8c3ce2875eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dd64d3081f49039e96f4b0f47bab0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b10ef9578d4bd49bd980167f32bf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7187b37645a4610a20bc2eaca0a352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7065c44d54944e2abd74e6193d8168fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daeaf8c3810a4ff5970429e6f529b22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "logits, activations = model.run_with_cache(\"Hello World my dear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db53ac4b64024d49adf2464cf091bc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"I am going to kill you for this outrageous comment by a Chicago conservative who unaccountably praised United Nations Secretary-General Ban Ki-Moon, over the question of reproductive procedures, on CNN's Reliable Sources.\\n\\nBan is a fantastic leader, I love him. He was appointed by the current U.S. president to serve his entire tenure of office in 2017/18, and it has over 14 months left. However, there are many around him who will say otherwise.\\n\\nI'd bet my nomination as president that if Ban\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"I am going to kill you for this outrageous\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 18565,   220]], device='cuda:0')\n",
      "tensor([[50256,    39,   378]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "positive_prompt = \"Love \"\n",
    "negative_prompt = \"Hate\"\n",
    "\n",
    "print(model.to_tokens(positive_prompt))\n",
    "print(model.to_tokens(negative_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, activations_love = model.run_with_cache(positive_prompt)\n",
    "_, activations_hate = model.run_with_cache(negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1600])\n",
      "torch.Size([3, 1600])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer_key = 'blocks.6.hook_resid_pre'\n",
    "print(activations_love[layer_key].shape)\n",
    "steering_vector = activations_love[layer_key][0] - activations_hate[layer_key][0]\n",
    "print(steering_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = \"I hate you because\"\n",
    "tokens= model.to_tokens(prompt)\n",
    "act_vector = steering_vector\n",
    "\n",
    "def head_ablation_hook(\n",
    "    value,\n",
    "    hook,\n",
    "    act_vector\n",
    "):\n",
    "    if value.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "\n",
    "    a = act_vector.shape[0]\n",
    "\n",
    "    print(f\"Shape of the value tensor: {value.shape} and shape of act: {act_vector.shape}\")\n",
    "    value[:, :a, :] +=  act_vector*4\n",
    "    print(f\"after Shape of the value tensor: {value.shape}\")\n",
    "    return value\n",
    "\n",
    "model.reset_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e2d94f14d54e92976cb5125c83a5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the value tensor: torch.Size([1, 5, 1600]) and shape of act: torch.Size([3, 1600])\n",
      "after Shape of the value tensor: torch.Size([1, 5, 1600])\n",
      "I hate you because you've don't bother to let your cousin know that the have high schooler listen, same as Yuigahama during the Tsushima trip, he messed it up because he hasn't had any Vlogs before. The other day, he\n"
     ]
    }
   ],
   "source": [
    "fwd_hooks=[(\n",
    "        layer_key, \n",
    "        partial(head_ablation_hook, act_vector=act_vector)\n",
    "        )]\n",
    "empty_hooks = []\n",
    "with model.hooks(fwd_hooks=fwd_hooks):\n",
    "    print(model.generate(prompt, max_new_tokens=50, do_sample=True))\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff87d2590ec4c3490f1356d4b1e3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"I am going to kill you for this outrageous award, right? Comparing Liverpool's youngsters to the team that has recently won the Premier League does seem completely done and batty and was the most common reason given by sports journalists & pundits in the hours & days leading up to this game a month ago. Yes of course Kolo Toure survived the incident and played, but in hindsight, the former Ivory Coast international's antics stole the day for Roberto Firmino. But fair enough.\\n\\nLiverpool attribute their semi-final victory in the\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
